{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c500ac0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/emnlp_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "#from roberta import RobertaForSequenceClassification\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Log in using your Hugging Face token\n",
        "login(\"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\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5247e6d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets  = load_dataset(\"glue\", 'mnli')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "074409bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "#from roberta import RobertaForSequenceClassification\n",
        "\n",
        "\n",
        "model_name = \"answerdotai/ModernBERT-base\"\n",
        "\n",
        "#config.num_labels=2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e0f5477",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "col_to_delete = ['question1','sentence2']\n",
        "\n",
        "def preprocessing_function(examples):\n",
        "    return tokenizer(examples['premise'], examples['hypothesis'])\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True)\n",
        "\n",
        "\n",
        "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "285ef2b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers.activations import ACT2FN\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to('cuda')\n",
        "\n",
        "import RoCoFT\n",
        "\n",
        "RoCoFT.PEFT(model, method='column', rank=3) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "09b86e50",
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "\n",
        "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    \n",
        "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
        "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
        "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
        "    accuracy = metrics.accuracy_score(labels, predictions)\n",
        "    \n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "de592e9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "import time\n",
        "from transformers import Trainer, TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='dir',\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    #gradient_accumulation_steps= 4,\n",
        "    weight_decay=0.00,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    save_steps=10000000,\n",
        "    logging_steps=500,\n",
        "   \n",
        "    load_best_model_at_end=True,\n",
        "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
        "    warmup_steps=100,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12ccd2fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24544' max='24544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24544/24544 1:31:34, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-score</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.041400</td>\n",
              "      <td>0.815153</td>\n",
              "      <td>0.641501</td>\n",
              "      <td>0.633106</td>\n",
              "      <td>0.631041</td>\n",
              "      <td>0.635863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.636400</td>\n",
              "      <td>0.535863</td>\n",
              "      <td>0.800501</td>\n",
              "      <td>0.790324</td>\n",
              "      <td>0.790710</td>\n",
              "      <td>0.789812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.513900</td>\n",
              "      <td>0.472986</td>\n",
              "      <td>0.814043</td>\n",
              "      <td>0.812572</td>\n",
              "      <td>0.811535</td>\n",
              "      <td>0.813958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.474100</td>\n",
              "      <td>0.468519</td>\n",
              "      <td>0.816791</td>\n",
              "      <td>0.814225</td>\n",
              "      <td>0.812562</td>\n",
              "      <td>0.815385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.448800</td>\n",
              "      <td>0.447085</td>\n",
              "      <td>0.828828</td>\n",
              "      <td>0.826290</td>\n",
              "      <td>0.824769</td>\n",
              "      <td>0.824962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.429700</td>\n",
              "      <td>0.417585</td>\n",
              "      <td>0.837885</td>\n",
              "      <td>0.835473</td>\n",
              "      <td>0.835737</td>\n",
              "      <td>0.835762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.433800</td>\n",
              "      <td>0.396457</td>\n",
              "      <td>0.847797</td>\n",
              "      <td>0.846200</td>\n",
              "      <td>0.846563</td>\n",
              "      <td>0.848192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.410600</td>\n",
              "      <td>0.398172</td>\n",
              "      <td>0.845409</td>\n",
              "      <td>0.845794</td>\n",
              "      <td>0.845491</td>\n",
              "      <td>0.846561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.409700</td>\n",
              "      <td>0.484013</td>\n",
              "      <td>0.828570</td>\n",
              "      <td>0.815967</td>\n",
              "      <td>0.813157</td>\n",
              "      <td>0.812430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.406200</td>\n",
              "      <td>0.378557</td>\n",
              "      <td>0.853269</td>\n",
              "      <td>0.852380</td>\n",
              "      <td>0.852530</td>\n",
              "      <td>0.854101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.406400</td>\n",
              "      <td>0.388578</td>\n",
              "      <td>0.854460</td>\n",
              "      <td>0.849756</td>\n",
              "      <td>0.850622</td>\n",
              "      <td>0.851554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.395100</td>\n",
              "      <td>0.381399</td>\n",
              "      <td>0.855000</td>\n",
              "      <td>0.852479</td>\n",
              "      <td>0.852675</td>\n",
              "      <td>0.852573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.393900</td>\n",
              "      <td>0.373472</td>\n",
              "      <td>0.859417</td>\n",
              "      <td>0.855547</td>\n",
              "      <td>0.856445</td>\n",
              "      <td>0.857769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.385138</td>\n",
              "      <td>0.854209</td>\n",
              "      <td>0.853628</td>\n",
              "      <td>0.852778</td>\n",
              "      <td>0.853082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.389000</td>\n",
              "      <td>0.374393</td>\n",
              "      <td>0.858436</td>\n",
              "      <td>0.857895</td>\n",
              "      <td>0.857451</td>\n",
              "      <td>0.857565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.387800</td>\n",
              "      <td>0.395450</td>\n",
              "      <td>0.850152</td>\n",
              "      <td>0.847633</td>\n",
              "      <td>0.846303</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>0.371653</td>\n",
              "      <td>0.859719</td>\n",
              "      <td>0.858555</td>\n",
              "      <td>0.858034</td>\n",
              "      <td>0.857972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.381800</td>\n",
              "      <td>0.362014</td>\n",
              "      <td>0.862509</td>\n",
              "      <td>0.862812</td>\n",
              "      <td>0.862411</td>\n",
              "      <td>0.863780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.377600</td>\n",
              "      <td>0.353914</td>\n",
              "      <td>0.862529</td>\n",
              "      <td>0.861807</td>\n",
              "      <td>0.862080</td>\n",
              "      <td>0.863067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.364400</td>\n",
              "      <td>0.361587</td>\n",
              "      <td>0.857653</td>\n",
              "      <td>0.857886</td>\n",
              "      <td>0.857070</td>\n",
              "      <td>0.858686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.371700</td>\n",
              "      <td>0.356509</td>\n",
              "      <td>0.860418</td>\n",
              "      <td>0.860813</td>\n",
              "      <td>0.860386</td>\n",
              "      <td>0.861131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.373200</td>\n",
              "      <td>0.346487</td>\n",
              "      <td>0.867312</td>\n",
              "      <td>0.867101</td>\n",
              "      <td>0.867083</td>\n",
              "      <td>0.867550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.370100</td>\n",
              "      <td>0.367864</td>\n",
              "      <td>0.862238</td>\n",
              "      <td>0.858456</td>\n",
              "      <td>0.858154</td>\n",
              "      <td>0.857463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.347356</td>\n",
              "      <td>0.866150</td>\n",
              "      <td>0.865044</td>\n",
              "      <td>0.865170</td>\n",
              "      <td>0.865410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.344800</td>\n",
              "      <td>0.371790</td>\n",
              "      <td>0.856491</td>\n",
              "      <td>0.856462</td>\n",
              "      <td>0.855293</td>\n",
              "      <td>0.856648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.325300</td>\n",
              "      <td>0.356072</td>\n",
              "      <td>0.862322</td>\n",
              "      <td>0.861644</td>\n",
              "      <td>0.861043</td>\n",
              "      <td>0.861131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.328800</td>\n",
              "      <td>0.345760</td>\n",
              "      <td>0.864989</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>0.865022</td>\n",
              "      <td>0.865716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.332300</td>\n",
              "      <td>0.357256</td>\n",
              "      <td>0.863464</td>\n",
              "      <td>0.861759</td>\n",
              "      <td>0.861148</td>\n",
              "      <td>0.860927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.335164</td>\n",
              "      <td>0.869210</td>\n",
              "      <td>0.868931</td>\n",
              "      <td>0.869046</td>\n",
              "      <td>0.869791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.322300</td>\n",
              "      <td>0.344119</td>\n",
              "      <td>0.865899</td>\n",
              "      <td>0.866300</td>\n",
              "      <td>0.865726</td>\n",
              "      <td>0.867040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>0.337519</td>\n",
              "      <td>0.869153</td>\n",
              "      <td>0.868949</td>\n",
              "      <td>0.869008</td>\n",
              "      <td>0.869689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.318400</td>\n",
              "      <td>0.345191</td>\n",
              "      <td>0.865503</td>\n",
              "      <td>0.865215</td>\n",
              "      <td>0.864736</td>\n",
              "      <td>0.865003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.320300</td>\n",
              "      <td>0.346894</td>\n",
              "      <td>0.864574</td>\n",
              "      <td>0.865040</td>\n",
              "      <td>0.864365</td>\n",
              "      <td>0.865104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>0.335163</td>\n",
              "      <td>0.870393</td>\n",
              "      <td>0.870533</td>\n",
              "      <td>0.870349</td>\n",
              "      <td>0.870912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.327800</td>\n",
              "      <td>0.337991</td>\n",
              "      <td>0.868874</td>\n",
              "      <td>0.869304</td>\n",
              "      <td>0.868809</td>\n",
              "      <td>0.869485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.313000</td>\n",
              "      <td>0.337486</td>\n",
              "      <td>0.869721</td>\n",
              "      <td>0.869211</td>\n",
              "      <td>0.869128</td>\n",
              "      <td>0.869384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>0.339815</td>\n",
              "      <td>0.870122</td>\n",
              "      <td>0.869751</td>\n",
              "      <td>0.869413</td>\n",
              "      <td>0.869587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.313100</td>\n",
              "      <td>0.334229</td>\n",
              "      <td>0.871627</td>\n",
              "      <td>0.871778</td>\n",
              "      <td>0.871578</td>\n",
              "      <td>0.872134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.319900</td>\n",
              "      <td>0.335022</td>\n",
              "      <td>0.872021</td>\n",
              "      <td>0.871786</td>\n",
              "      <td>0.871715</td>\n",
              "      <td>0.872134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.314600</td>\n",
              "      <td>0.336641</td>\n",
              "      <td>0.870503</td>\n",
              "      <td>0.870934</td>\n",
              "      <td>0.870595</td>\n",
              "      <td>0.871319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.313100</td>\n",
              "      <td>0.331956</td>\n",
              "      <td>0.873477</td>\n",
              "      <td>0.873561</td>\n",
              "      <td>0.873502</td>\n",
              "      <td>0.874478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.311700</td>\n",
              "      <td>0.333009</td>\n",
              "      <td>0.871990</td>\n",
              "      <td>0.872365</td>\n",
              "      <td>0.872120</td>\n",
              "      <td>0.873051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.334647</td>\n",
              "      <td>0.872468</td>\n",
              "      <td>0.872470</td>\n",
              "      <td>0.872289</td>\n",
              "      <td>0.872746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.317800</td>\n",
              "      <td>0.333524</td>\n",
              "      <td>0.871019</td>\n",
              "      <td>0.871316</td>\n",
              "      <td>0.871060</td>\n",
              "      <td>0.871727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.333507</td>\n",
              "      <td>0.872530</td>\n",
              "      <td>0.872700</td>\n",
              "      <td>0.872489</td>\n",
              "      <td>0.873051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.308100</td>\n",
              "      <td>0.332907</td>\n",
              "      <td>0.871629</td>\n",
              "      <td>0.871814</td>\n",
              "      <td>0.871624</td>\n",
              "      <td>0.872236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.313200</td>\n",
              "      <td>0.333850</td>\n",
              "      <td>0.871842</td>\n",
              "      <td>0.872086</td>\n",
              "      <td>0.871839</td>\n",
              "      <td>0.872440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.317600</td>\n",
              "      <td>0.334074</td>\n",
              "      <td>0.872292</td>\n",
              "      <td>0.872378</td>\n",
              "      <td>0.872162</td>\n",
              "      <td>0.872644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.307700</td>\n",
              "      <td>0.334067</td>\n",
              "      <td>0.872278</td>\n",
              "      <td>0.872367</td>\n",
              "      <td>0.872155</td>\n",
              "      <td>0.872644</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=24544, training_loss=0.37831873894514906, metrics={'train_runtime': 5497.0925, 'train_samples_per_second': 142.876, 'train_steps_per_second': 4.465, 'total_flos': 193231898885376.0, 'train_loss': 0.37831873894514906, 'epoch': 2.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76b7afa",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "emnlp_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
