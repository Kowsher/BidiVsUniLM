{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b47c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 14.4M/14.4M [00:00<00:00, 220MB/s]\n",
      "Generating train split: 33410 examples [00:00, 95411.22 examples/s]\n",
      "Downloading data: 100%|██████████| 844k/844k [00:00<00:00, 145MB/s]\n",
      "Generating train split: 1954 examples [00:00, 159712.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#load train data\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset('json', data_files='https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/social_i_qa/train.json')\n",
    "\n",
    "val_datasets = load_dataset('json', data_files='https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/social_i_qa/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7756a810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 1954\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769a952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and fit label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(raw_datasets['train']['answer'])\n",
    "# Create the integer labels\n",
    "train_labels = label_encoder.transform(raw_datasets['train']['answer'])\n",
    "\n",
    "# Add a new 'labels' column\n",
    "raw_datasets['train'] = raw_datasets['train'].add_column('labels', train_labels)\n",
    "\n",
    "val_labels = label_encoder.transform(val_datasets['train']['answer'])\n",
    "\n",
    "# Add a new 'labels' column\n",
    "val_datasets['train'] = val_datasets['train'].add_column('labels', val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasets['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de228bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed721fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 33410/33410 [00:02<00:00, 15187.25 examples/s]\n",
      "Map: 100%|██████████| 1954/1954 [00:00<00:00, 16549.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'answer', 'labels'],\n",
      "    num_rows: 33410\n",
      "})\n",
      "Validation Dataset: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'answer', 'labels'],\n",
      "    num_rows: 1954\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    return f\"\"\"# input: {data_point[\"instruction\"].split('format:')[0]}:\"\"\"\n",
    "               \n",
    "\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "def add_label_column(example):\n",
    "\n",
    "    example['labels'] = example['labels']\n",
    "  \n",
    "    example['input'] = generate_prompt(example)\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Map the function over train and validation datasets\n",
    "\n",
    "train_data = raw_datasets['train'].map(add_label_column)\n",
    "val_data = val_datasets['train'].map(add_label_column)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "\n",
    "# Inspect the updated datasets\n",
    "print(\"Train Dataset:\", train_data)\n",
    "print(\"Validation Dataset:\", val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e33204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# input: Please choose the correct answer to the question: Cameron decided to have a barbecue and gathered her friends together. How would Others feel as a result?\\n\\nAnswer1: like attending Answer2: like staying home Answer3: a good friend to have\\n\\nAnswer :'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fde6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 33410/33410 [00:02<00:00, 11233.19 examples/s]\n",
      "Map: 100%|██████████| 1954/1954 [00:00<00:00, 12383.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete =  ['instruction', 'input', 'output', 'answer']\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "def preprocessing_function(examples):\n",
    "   \n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_data = train_data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_val_data = val_data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_train_data.set_format(\"torch\")\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1931ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] # input: Please choose the correct answer to the question: Sydney was a school teacher and made sure their students learned well. How would you describe Sydney? Answer1: As someone that asked for a job Answer2: As someone that takes teaching seriously Answer3: Like a leader Answer :[SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train_data['input_ids'][10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6b985",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25900f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'answer', 'labels'],\n",
       "    num_rows: 1954\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fdaa612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lengths = [len(ids) for ids in tokenized_train_data['input_ids']]\n",
    "mx = max(all_lengths)\n",
    "mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6618d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = sum(len(ids) > 512 for ids in tokenized_train_data['input_ids'])\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1005af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels (classes): 3\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"Total labels (classes): {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a46cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "# from modeling import MLMSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_labels\n",
    "config.mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864ccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RoCoFT\n",
    "\n",
    "RoCoFT.PEFT(model, method='column', rank=3) \n",
    "#targets=['key', 'value', 'dense', 'query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef34afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dbcf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.0,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "557cdbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8356' max='8356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8356/8356 29:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.108700</td>\n",
       "      <td>1.108163</td>\n",
       "      <td>0.225729</td>\n",
       "      <td>0.337930</td>\n",
       "      <td>0.239558</td>\n",
       "      <td>0.338280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.099000</td>\n",
       "      <td>1.102499</td>\n",
       "      <td>0.556239</td>\n",
       "      <td>0.334359</td>\n",
       "      <td>0.169480</td>\n",
       "      <td>0.335722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.105500</td>\n",
       "      <td>1.099133</td>\n",
       "      <td>0.360597</td>\n",
       "      <td>0.337024</td>\n",
       "      <td>0.218869</td>\n",
       "      <td>0.338280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.100900</td>\n",
       "      <td>1.098166</td>\n",
       "      <td>0.328330</td>\n",
       "      <td>0.332408</td>\n",
       "      <td>0.283601</td>\n",
       "      <td>0.332139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.104800</td>\n",
       "      <td>1.098469</td>\n",
       "      <td>0.340117</td>\n",
       "      <td>0.335023</td>\n",
       "      <td>0.286294</td>\n",
       "      <td>0.336745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.101500</td>\n",
       "      <td>1.098791</td>\n",
       "      <td>0.337422</td>\n",
       "      <td>0.327770</td>\n",
       "      <td>0.270733</td>\n",
       "      <td>0.327533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.099800</td>\n",
       "      <td>1.099836</td>\n",
       "      <td>0.250284</td>\n",
       "      <td>0.330284</td>\n",
       "      <td>0.264160</td>\n",
       "      <td>0.328557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.100100</td>\n",
       "      <td>1.098777</td>\n",
       "      <td>0.329828</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>0.296379</td>\n",
       "      <td>0.327533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.100900</td>\n",
       "      <td>1.100552</td>\n",
       "      <td>0.346767</td>\n",
       "      <td>0.342230</td>\n",
       "      <td>0.321299</td>\n",
       "      <td>0.342375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.104100</td>\n",
       "      <td>1.101267</td>\n",
       "      <td>0.325664</td>\n",
       "      <td>0.329154</td>\n",
       "      <td>0.292379</td>\n",
       "      <td>0.328557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.104300</td>\n",
       "      <td>1.099188</td>\n",
       "      <td>0.321174</td>\n",
       "      <td>0.333397</td>\n",
       "      <td>0.189984</td>\n",
       "      <td>0.334698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>1.099040</td>\n",
       "      <td>0.336210</td>\n",
       "      <td>0.336416</td>\n",
       "      <td>0.296621</td>\n",
       "      <td>0.337257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.097600</td>\n",
       "      <td>1.099711</td>\n",
       "      <td>0.313337</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>0.239461</td>\n",
       "      <td>0.337769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.101700</td>\n",
       "      <td>1.099026</td>\n",
       "      <td>0.341433</td>\n",
       "      <td>0.338604</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.339816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.097700</td>\n",
       "      <td>1.099883</td>\n",
       "      <td>0.215318</td>\n",
       "      <td>0.324871</td>\n",
       "      <td>0.257331</td>\n",
       "      <td>0.323951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.102100</td>\n",
       "      <td>1.099575</td>\n",
       "      <td>0.223983</td>\n",
       "      <td>0.335768</td>\n",
       "      <td>0.257457</td>\n",
       "      <td>0.333675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.099500</td>\n",
       "      <td>1.099234</td>\n",
       "      <td>0.215621</td>\n",
       "      <td>0.327556</td>\n",
       "      <td>0.234677</td>\n",
       "      <td>0.329069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.098900</td>\n",
       "      <td>1.098964</td>\n",
       "      <td>0.296664</td>\n",
       "      <td>0.333975</td>\n",
       "      <td>0.224617</td>\n",
       "      <td>0.330604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.099200</td>\n",
       "      <td>1.098521</td>\n",
       "      <td>0.282533</td>\n",
       "      <td>0.336391</td>\n",
       "      <td>0.272990</td>\n",
       "      <td>0.334698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.098500</td>\n",
       "      <td>1.100982</td>\n",
       "      <td>0.248788</td>\n",
       "      <td>0.339258</td>\n",
       "      <td>0.195409</td>\n",
       "      <td>0.335210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.099800</td>\n",
       "      <td>1.098923</td>\n",
       "      <td>0.190464</td>\n",
       "      <td>0.331373</td>\n",
       "      <td>0.175088</td>\n",
       "      <td>0.332651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.102900</td>\n",
       "      <td>1.098640</td>\n",
       "      <td>0.337291</td>\n",
       "      <td>0.327381</td>\n",
       "      <td>0.276336</td>\n",
       "      <td>0.328557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.099500</td>\n",
       "      <td>1.099877</td>\n",
       "      <td>0.230492</td>\n",
       "      <td>0.337889</td>\n",
       "      <td>0.205510</td>\n",
       "      <td>0.340328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.102400</td>\n",
       "      <td>1.099504</td>\n",
       "      <td>0.317594</td>\n",
       "      <td>0.324270</td>\n",
       "      <td>0.302461</td>\n",
       "      <td>0.324463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.098200</td>\n",
       "      <td>1.101647</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.332815</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.328557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.103100</td>\n",
       "      <td>1.099631</td>\n",
       "      <td>0.220448</td>\n",
       "      <td>0.336469</td>\n",
       "      <td>0.235728</td>\n",
       "      <td>0.333163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.099900</td>\n",
       "      <td>1.099734</td>\n",
       "      <td>0.242583</td>\n",
       "      <td>0.334327</td>\n",
       "      <td>0.177372</td>\n",
       "      <td>0.335722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.099400</td>\n",
       "      <td>1.099641</td>\n",
       "      <td>0.352486</td>\n",
       "      <td>0.343068</td>\n",
       "      <td>0.289947</td>\n",
       "      <td>0.341863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.099900</td>\n",
       "      <td>1.099925</td>\n",
       "      <td>0.221566</td>\n",
       "      <td>0.331503</td>\n",
       "      <td>0.223231</td>\n",
       "      <td>0.328045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.097500</td>\n",
       "      <td>1.099327</td>\n",
       "      <td>0.227406</td>\n",
       "      <td>0.334165</td>\n",
       "      <td>0.221217</td>\n",
       "      <td>0.330604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.100500</td>\n",
       "      <td>1.099116</td>\n",
       "      <td>0.352953</td>\n",
       "      <td>0.347317</td>\n",
       "      <td>0.289044</td>\n",
       "      <td>0.344933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.100200</td>\n",
       "      <td>1.100188</td>\n",
       "      <td>0.111566</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.167178</td>\n",
       "      <td>0.334698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.101200</td>\n",
       "      <td>1.099123</td>\n",
       "      <td>0.307129</td>\n",
       "      <td>0.331586</td>\n",
       "      <td>0.196513</td>\n",
       "      <td>0.332651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.099200</td>\n",
       "      <td>1.098905</td>\n",
       "      <td>0.286919</td>\n",
       "      <td>0.335919</td>\n",
       "      <td>0.260439</td>\n",
       "      <td>0.335722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.102700</td>\n",
       "      <td>1.098318</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.232545</td>\n",
       "      <td>0.344933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.101100</td>\n",
       "      <td>1.098667</td>\n",
       "      <td>0.330958</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.231838</td>\n",
       "      <td>0.332139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.098200</td>\n",
       "      <td>1.099058</td>\n",
       "      <td>0.342708</td>\n",
       "      <td>0.332346</td>\n",
       "      <td>0.176046</td>\n",
       "      <td>0.333675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.098000</td>\n",
       "      <td>1.100495</td>\n",
       "      <td>0.229335</td>\n",
       "      <td>0.332915</td>\n",
       "      <td>0.201776</td>\n",
       "      <td>0.329069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.099600</td>\n",
       "      <td>1.097364</td>\n",
       "      <td>0.368750</td>\n",
       "      <td>0.369856</td>\n",
       "      <td>0.363154</td>\n",
       "      <td>0.369498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.097300</td>\n",
       "      <td>1.098727</td>\n",
       "      <td>0.339965</td>\n",
       "      <td>0.362211</td>\n",
       "      <td>0.290472</td>\n",
       "      <td>0.360287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.096600</td>\n",
       "      <td>1.094740</td>\n",
       "      <td>0.410430</td>\n",
       "      <td>0.374875</td>\n",
       "      <td>0.343547</td>\n",
       "      <td>0.376663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.090200</td>\n",
       "      <td>1.090915</td>\n",
       "      <td>0.396880</td>\n",
       "      <td>0.393051</td>\n",
       "      <td>0.314922</td>\n",
       "      <td>0.390481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>1.092517</td>\n",
       "      <td>0.252922</td>\n",
       "      <td>0.385664</td>\n",
       "      <td>0.296796</td>\n",
       "      <td>0.382805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>1.060442</td>\n",
       "      <td>0.424429</td>\n",
       "      <td>0.414866</td>\n",
       "      <td>0.410149</td>\n",
       "      <td>0.414534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.000500</td>\n",
       "      <td>1.103970</td>\n",
       "      <td>0.449494</td>\n",
       "      <td>0.407229</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.404299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>1.040549</td>\n",
       "      <td>0.423177</td>\n",
       "      <td>0.445692</td>\n",
       "      <td>0.399209</td>\n",
       "      <td>0.444217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>1.045367</td>\n",
       "      <td>0.435741</td>\n",
       "      <td>0.440547</td>\n",
       "      <td>0.359658</td>\n",
       "      <td>0.438588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.964400</td>\n",
       "      <td>1.033399</td>\n",
       "      <td>0.442844</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>0.442539</td>\n",
       "      <td>0.451894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.945700</td>\n",
       "      <td>1.036544</td>\n",
       "      <td>0.445040</td>\n",
       "      <td>0.451621</td>\n",
       "      <td>0.426963</td>\n",
       "      <td>0.450870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.951200</td>\n",
       "      <td>1.055067</td>\n",
       "      <td>0.420316</td>\n",
       "      <td>0.435969</td>\n",
       "      <td>0.410908</td>\n",
       "      <td>0.433982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.963300</td>\n",
       "      <td>1.028255</td>\n",
       "      <td>0.474217</td>\n",
       "      <td>0.450554</td>\n",
       "      <td>0.363953</td>\n",
       "      <td>0.448823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>1.057618</td>\n",
       "      <td>0.455140</td>\n",
       "      <td>0.443666</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.441146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.928900</td>\n",
       "      <td>1.027833</td>\n",
       "      <td>0.445030</td>\n",
       "      <td>0.458639</td>\n",
       "      <td>0.424461</td>\n",
       "      <td>0.457523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>1.033772</td>\n",
       "      <td>0.436542</td>\n",
       "      <td>0.455457</td>\n",
       "      <td>0.381096</td>\n",
       "      <td>0.453429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.939900</td>\n",
       "      <td>1.029763</td>\n",
       "      <td>0.442319</td>\n",
       "      <td>0.457953</td>\n",
       "      <td>0.417274</td>\n",
       "      <td>0.455988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.945600</td>\n",
       "      <td>1.023509</td>\n",
       "      <td>0.447646</td>\n",
       "      <td>0.461099</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.459570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>1.014135</td>\n",
       "      <td>0.455736</td>\n",
       "      <td>0.464855</td>\n",
       "      <td>0.450192</td>\n",
       "      <td>0.463664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>1.009637</td>\n",
       "      <td>0.458624</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>0.455032</td>\n",
       "      <td>0.470317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>1.017981</td>\n",
       "      <td>0.447539</td>\n",
       "      <td>0.467114</td>\n",
       "      <td>0.411849</td>\n",
       "      <td>0.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.913400</td>\n",
       "      <td>1.022579</td>\n",
       "      <td>0.451482</td>\n",
       "      <td>0.461886</td>\n",
       "      <td>0.452177</td>\n",
       "      <td>0.460594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>1.026183</td>\n",
       "      <td>0.459803</td>\n",
       "      <td>0.473481</td>\n",
       "      <td>0.456371</td>\n",
       "      <td>0.471853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>1.021791</td>\n",
       "      <td>0.457038</td>\n",
       "      <td>0.470247</td>\n",
       "      <td>0.454802</td>\n",
       "      <td>0.468782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.931500</td>\n",
       "      <td>1.021087</td>\n",
       "      <td>0.457825</td>\n",
       "      <td>0.471418</td>\n",
       "      <td>0.457004</td>\n",
       "      <td>0.469806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>1.015949</td>\n",
       "      <td>0.462802</td>\n",
       "      <td>0.474259</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>0.472876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.899700</td>\n",
       "      <td>1.032432</td>\n",
       "      <td>0.449376</td>\n",
       "      <td>0.468090</td>\n",
       "      <td>0.443334</td>\n",
       "      <td>0.466223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>1.030167</td>\n",
       "      <td>0.453293</td>\n",
       "      <td>0.470135</td>\n",
       "      <td>0.441861</td>\n",
       "      <td>0.468270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>1.030657</td>\n",
       "      <td>0.453062</td>\n",
       "      <td>0.469659</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.467758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>1.031548</td>\n",
       "      <td>0.458694</td>\n",
       "      <td>0.472762</td>\n",
       "      <td>0.435679</td>\n",
       "      <td>0.470829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>1.038032</td>\n",
       "      <td>0.455418</td>\n",
       "      <td>0.470630</td>\n",
       "      <td>0.449672</td>\n",
       "      <td>0.468782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>1.031470</td>\n",
       "      <td>0.456072</td>\n",
       "      <td>0.471510</td>\n",
       "      <td>0.453167</td>\n",
       "      <td>0.469806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>1.036662</td>\n",
       "      <td>0.457748</td>\n",
       "      <td>0.471509</td>\n",
       "      <td>0.455049</td>\n",
       "      <td>0.469806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.924600</td>\n",
       "      <td>1.029544</td>\n",
       "      <td>0.462486</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.460358</td>\n",
       "      <td>0.474411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>1.027800</td>\n",
       "      <td>0.462099</td>\n",
       "      <td>0.475537</td>\n",
       "      <td>0.459638</td>\n",
       "      <td>0.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>1.032521</td>\n",
       "      <td>0.458813</td>\n",
       "      <td>0.473043</td>\n",
       "      <td>0.456752</td>\n",
       "      <td>0.471341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>1.027823</td>\n",
       "      <td>0.462898</td>\n",
       "      <td>0.475974</td>\n",
       "      <td>0.462730</td>\n",
       "      <td>0.474411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>1.032452</td>\n",
       "      <td>0.459970</td>\n",
       "      <td>0.472958</td>\n",
       "      <td>0.457909</td>\n",
       "      <td>0.471341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>1.029654</td>\n",
       "      <td>0.461779</td>\n",
       "      <td>0.474436</td>\n",
       "      <td>0.460476</td>\n",
       "      <td>0.472876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>1.027727</td>\n",
       "      <td>0.464311</td>\n",
       "      <td>0.476983</td>\n",
       "      <td>0.463820</td>\n",
       "      <td>0.475435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.903900</td>\n",
       "      <td>1.026467</td>\n",
       "      <td>0.463482</td>\n",
       "      <td>0.476481</td>\n",
       "      <td>0.463336</td>\n",
       "      <td>0.474923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.908700</td>\n",
       "      <td>1.025365</td>\n",
       "      <td>0.462807</td>\n",
       "      <td>0.475963</td>\n",
       "      <td>0.463030</td>\n",
       "      <td>0.474411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>1.025608</td>\n",
       "      <td>0.463948</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>0.464048</td>\n",
       "      <td>0.475435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>1.025270</td>\n",
       "      <td>0.462881</td>\n",
       "      <td>0.475959</td>\n",
       "      <td>0.463053</td>\n",
       "      <td>0.474411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>1.025347</td>\n",
       "      <td>0.462881</td>\n",
       "      <td>0.475959</td>\n",
       "      <td>0.463053</td>\n",
       "      <td>0.474411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/emnlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8356, training_loss=1.0156582480358016, metrics={'train_runtime': 1765.2645, 'train_samples_per_second': 75.705, 'train_steps_per_second': 4.734, 'total_flos': 22671804325392.0, 'train_loss': 1.0156582480358016, 'epoch': 4.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b8833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 20983.85 examples/s]\n",
      "100%|██████████| 500/500 [00:49<00:00, 10.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Define preprocessing function with truncation\n",
    "def preprocessing_function(examples):\n",
    "    # Adjust 'text' to your input column name\n",
    "    return tokenizer(\n",
    "        examples['input'],\n",
    "        padding=False,  # Padding handled by collator\n",
    "        truncation=True,\n",
    "        max_length=512  # Set to your model's max length\n",
    "    )\n",
    "\n",
    "# Apply preprocessing with truncation\n",
    "col_to_delete=['instruction', 'input', 'output', 'answer']\n",
    "tokenized_train_data1 = train_data.select(range(2000)).map(\n",
    "    preprocessing_function,\n",
    "    batched=True,\n",
    "    remove_columns=col_to_delete\n",
    ")\n",
    "\n",
    "# Set data collator with explicit max_length\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512  # Match tokenizer's max_length\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    tokenized_train_data1,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Proceed with model evaluation and feature extraction\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "X_list = []\n",
    "Z_layer_outputs = [[] for _ in range(model.config.num_hidden_layers)]\n",
    "Y_list = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].cuda()\n",
    "    attention_mask = batch['attention_mask'].cuda()\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        \n",
    "        # Store embeddings (mean pooled)\n",
    "        X_list.append(hidden_states[0].mean(dim=1).cpu())\n",
    "        \n",
    "        # Store layer outputs (mean pooled)\n",
    "        for i, layer_out in enumerate(hidden_states[1:]):\n",
    "            Z_layer_outputs[i].append(layer_out.mean(dim=1).cpu())\n",
    "        \n",
    "        Y_list.append(labels)\n",
    "\n",
    "# Stack all tensors\n",
    "X_tensor = torch.cat(X_list, dim=0)\n",
    "Z_tensors = [torch.cat(layer, dim=0) for layer in Z_layer_outputs]\n",
    "Z_tensor = torch.stack(Z_tensors, dim=0)\n",
    "Y_tensor = torch.cat(Y_list, dim=0)\n",
    "\n",
    "# Save dataset\n",
    "dataset_dict = {'X': X_tensor, 'Z': Z_tensor, 'Y': Y_tensor}\n",
    "with open('full_dataset_hws_mlm.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
