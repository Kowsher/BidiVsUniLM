{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b47c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'corpus', 'sentence', 'token', 'complexity'],\n",
      "        num_rows: 7232\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'corpus', 'sentence', 'token', 'complexity'],\n",
      "        num_rows: 887\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# URL of the TSV file\n",
    "url = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv\"\n",
    "test_url = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/refs/heads/master/test-labels/lcp_single_test.tsv\"\n",
    "# Load the TSV file using the csv format\n",
    "train_data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=url,\n",
    "    delimiter=\"\\t\"  # Specify tab-separated values\n",
    ")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(train_data)\n",
    "\n",
    "\n",
    "val_data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=test_url,\n",
    "    delimiter=\"\\t\"  # Specify tab-separated values\n",
    ")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de228bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp_2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed721fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a prompt for evaluating the humor intensity of an edited headline.\n",
    "    Args:\n",
    "        data_point (dict): A dictionary containing 'original', 'edit', and 'meanGrade'.\n",
    "    Returns:\n",
    "        str: The formatted prompt as a string.\n",
    "    \"\"\"\n",
    "    return f\"\"\"# Sentence: {data_point['sentence']} # Word: {data_point['token']} # Output: The complexity score between word and output is\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "def add_label_column(example):\n",
    "\n",
    "    example['labels'] = float(example['complexity'])\n",
    "  \n",
    "    example['input'] = generate_prompt(example)\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "train_data = train_data['train'].map(add_label_column)\n",
    "val_data = val_data['train'].map(add_label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33204c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fde6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7232/7232 [00:00<00:00, 13319.38 examples/s]\n",
      "Map: 100%|██████████| 887/887 [00:00<00:00, 13614.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete = ['id', 'corpus', 'sentence', 'token', 'complexity']\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "def preprocessing_function(examples):\n",
    "   \n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_data = train_data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_val_data = val_data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_train_data.set_format(\"torch\")\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1931ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] # Sentence: Seven days you shall eat unleavened bread, as I commanded you, at the time appointed in the month Abib; for in the month Abib you came out from Egypt. # Word: days # Output: The complexity score between word and output is[SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train_data['input_ids'][10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6b985",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25900f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'corpus', 'sentence', 'token', 'complexity', 'labels', 'input'],\n",
       "    num_rows: 887\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdaa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lengths = [len(ids) for ids in tokenized_train_data['input_ids']]\n",
    "mx = max(all_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6618d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = sum(len(ids) > 512 for ids in tokenized_train_data['input_ids'])\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a46cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "# from modeling import MLMSequenceClassification\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.mask_token_id = 50264\n",
    "config.num_labels = 1\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "159b238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864ccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RoCoFT\n",
    "\n",
    "RoCoFT.PEFT(model, method='column', rank=3) \n",
    "#targets=['key', 'value', 'dense', 'query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef34afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # If predictions are logits or have extra dimensions, squeeze\n",
    "    if predictions.ndim > 1:\n",
    "        predictions = predictions.squeeze()\n",
    "\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    \n",
    "    # Define an \"accuracy\" for regression:\n",
    "    # Example: within some threshold tolerance\n",
    "    tolerance = 0.1  # you can change this\n",
    "    acc = np.mean(np.abs(predictions - labels) < tolerance)\n",
    "\n",
    "    pearson_corr, _ = pearsonr(predictions, labels)\n",
    "    spearman_corr, _ = spearmanr(predictions, labels)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Accuracy\": acc,\n",
    "        \"R2\": r2,\n",
    "        \"Pearson\": pearson_corr,\n",
    "        \"Spearman's Rank\": spearman_corr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dbcf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.20,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557cdbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9040' max='9040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9040/9040 18:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>R2</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>0.101511</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>0.135047</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.112206</td>\n",
       "      <td>0.051592</td>\n",
       "      <td>0.055299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.101968</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.130980</td>\n",
       "      <td>0.567080</td>\n",
       "      <td>-0.046224</td>\n",
       "      <td>0.053582</td>\n",
       "      <td>0.031569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.016895</td>\n",
       "      <td>0.104164</td>\n",
       "      <td>0.016895</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.544532</td>\n",
       "      <td>-0.030312</td>\n",
       "      <td>0.087624</td>\n",
       "      <td>0.077932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.097279</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.131247</td>\n",
       "      <td>0.650507</td>\n",
       "      <td>-0.050500</td>\n",
       "      <td>0.068595</td>\n",
       "      <td>0.066997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.016909</td>\n",
       "      <td>0.096782</td>\n",
       "      <td>0.016909</td>\n",
       "      <td>0.130034</td>\n",
       "      <td>0.657272</td>\n",
       "      <td>-0.031174</td>\n",
       "      <td>0.114184</td>\n",
       "      <td>0.107782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.100791</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.128172</td>\n",
       "      <td>0.580609</td>\n",
       "      <td>-0.001847</td>\n",
       "      <td>0.116443</td>\n",
       "      <td>0.123018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.099557</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.127478</td>\n",
       "      <td>0.590755</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>0.135758</td>\n",
       "      <td>0.130012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>0.096840</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>0.130513</td>\n",
       "      <td>0.661781</td>\n",
       "      <td>-0.038776</td>\n",
       "      <td>0.158703</td>\n",
       "      <td>0.136752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.101789</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.128724</td>\n",
       "      <td>0.570462</td>\n",
       "      <td>-0.010503</td>\n",
       "      <td>0.160043</td>\n",
       "      <td>0.140399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>0.109151</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>0.133780</td>\n",
       "      <td>0.489290</td>\n",
       "      <td>-0.091433</td>\n",
       "      <td>0.163575</td>\n",
       "      <td>0.136589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.102084</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.129325</td>\n",
       "      <td>0.576099</td>\n",
       "      <td>-0.019954</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>-0.023266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>0.127707</td>\n",
       "      <td>0.648253</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.181934</td>\n",
       "      <td>0.168067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.016455</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.016455</td>\n",
       "      <td>0.128277</td>\n",
       "      <td>0.582864</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>-0.014593</td>\n",
       "      <td>0.003852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.098149</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.128086</td>\n",
       "      <td>0.624577</td>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-0.088823</td>\n",
       "      <td>-0.079585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.098544</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.128054</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.007572</td>\n",
       "      <td>-0.008087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.099098</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>-0.096111</td>\n",
       "      <td>-0.088658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.101327</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.128939</td>\n",
       "      <td>0.587373</td>\n",
       "      <td>-0.013879</td>\n",
       "      <td>-0.043068</td>\n",
       "      <td>-0.041747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>0.103301</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>0.130013</td>\n",
       "      <td>0.543405</td>\n",
       "      <td>-0.030839</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>0.009801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.103197</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.129953</td>\n",
       "      <td>0.555806</td>\n",
       "      <td>-0.029880</td>\n",
       "      <td>-0.025339</td>\n",
       "      <td>-0.041791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>0.097468</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>0.128420</td>\n",
       "      <td>0.627959</td>\n",
       "      <td>-0.005732</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.032473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.107238</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>0.529876</td>\n",
       "      <td>-0.072644</td>\n",
       "      <td>0.052947</td>\n",
       "      <td>0.086629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>0.100771</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>0.128677</td>\n",
       "      <td>0.586246</td>\n",
       "      <td>-0.009754</td>\n",
       "      <td>0.043461</td>\n",
       "      <td>0.051678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.097695</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.128253</td>\n",
       "      <td>0.627959</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>-0.036734</td>\n",
       "      <td>-0.024561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>0.097315</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>0.128594</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>-0.008452</td>\n",
       "      <td>0.049066</td>\n",
       "      <td>0.032607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.099487</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.593010</td>\n",
       "      <td>-0.002365</td>\n",
       "      <td>-0.048673</td>\n",
       "      <td>-0.046448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.097367</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.132258</td>\n",
       "      <td>0.661781</td>\n",
       "      <td>-0.066743</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.021842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>0.105217</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>0.131212</td>\n",
       "      <td>0.532131</td>\n",
       "      <td>-0.049935</td>\n",
       "      <td>-0.013714</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.097139</td>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>0.667418</td>\n",
       "      <td>-0.049929</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.003931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.103670</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.130232</td>\n",
       "      <td>0.546787</td>\n",
       "      <td>-0.034310</td>\n",
       "      <td>0.108366</td>\n",
       "      <td>0.117365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.109060</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.133990</td>\n",
       "      <td>0.484780</td>\n",
       "      <td>-0.094870</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.027256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.097791</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.128203</td>\n",
       "      <td>0.629087</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.025735</td>\n",
       "      <td>0.046036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.130865</td>\n",
       "      <td>0.664036</td>\n",
       "      <td>-0.044398</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.006309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.016549</td>\n",
       "      <td>0.097277</td>\n",
       "      <td>0.016549</td>\n",
       "      <td>0.128644</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>-0.009248</td>\n",
       "      <td>-0.022953</td>\n",
       "      <td>-0.049307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.097663</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.128272</td>\n",
       "      <td>0.632469</td>\n",
       "      <td>-0.003415</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>0.072966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.016497</td>\n",
       "      <td>0.097447</td>\n",
       "      <td>0.016497</td>\n",
       "      <td>0.128442</td>\n",
       "      <td>0.627959</td>\n",
       "      <td>-0.006069</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.009585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.105796</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.131603</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>-0.056209</td>\n",
       "      <td>-0.002596</td>\n",
       "      <td>0.013764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.098664</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.128059</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.002851</td>\n",
       "      <td>-0.023065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>0.097111</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>0.128935</td>\n",
       "      <td>0.644870</td>\n",
       "      <td>-0.013813</td>\n",
       "      <td>0.017480</td>\n",
       "      <td>0.014695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.098396</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.128056</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.015030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.016697</td>\n",
       "      <td>0.097014</td>\n",
       "      <td>0.016697</td>\n",
       "      <td>0.129218</td>\n",
       "      <td>0.648253</td>\n",
       "      <td>-0.018264</td>\n",
       "      <td>-0.020202</td>\n",
       "      <td>-0.031228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.098668</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.128059</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>0.029618</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.098638</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.128057</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.092351</td>\n",
       "      <td>-0.088611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.097649</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.128281</td>\n",
       "      <td>0.632469</td>\n",
       "      <td>-0.003554</td>\n",
       "      <td>-0.010799</td>\n",
       "      <td>0.013737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.107606</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.132892</td>\n",
       "      <td>0.525366</td>\n",
       "      <td>-0.076995</td>\n",
       "      <td>0.017833</td>\n",
       "      <td>0.022850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>0.096960</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>0.130019</td>\n",
       "      <td>0.665163</td>\n",
       "      <td>-0.030936</td>\n",
       "      <td>0.065762</td>\n",
       "      <td>0.093288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.016568</td>\n",
       "      <td>0.100863</td>\n",
       "      <td>0.016568</td>\n",
       "      <td>0.128719</td>\n",
       "      <td>0.586246</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.059937</td>\n",
       "      <td>0.068359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>0.099335</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>0.128166</td>\n",
       "      <td>0.593010</td>\n",
       "      <td>-0.001751</td>\n",
       "      <td>-0.046014</td>\n",
       "      <td>-0.067916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.106777</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.132285</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>-0.067181</td>\n",
       "      <td>0.077757</td>\n",
       "      <td>0.072018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.097971</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.128131</td>\n",
       "      <td>0.627959</td>\n",
       "      <td>-0.001216</td>\n",
       "      <td>0.037055</td>\n",
       "      <td>0.041782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.099803</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.589628</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.042723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.017068</td>\n",
       "      <td>0.104337</td>\n",
       "      <td>0.017068</td>\n",
       "      <td>0.130645</td>\n",
       "      <td>0.532131</td>\n",
       "      <td>-0.040883</td>\n",
       "      <td>0.035341</td>\n",
       "      <td>0.025448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.016445</td>\n",
       "      <td>0.097723</td>\n",
       "      <td>0.016445</td>\n",
       "      <td>0.128236</td>\n",
       "      <td>0.629087</td>\n",
       "      <td>-0.002857</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.022781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.099485</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.128204</td>\n",
       "      <td>0.593010</td>\n",
       "      <td>-0.002357</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.024248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.098305</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.128062</td>\n",
       "      <td>0.615558</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.057541</td>\n",
       "      <td>-0.031852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.097543</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.128354</td>\n",
       "      <td>0.629087</td>\n",
       "      <td>-0.004695</td>\n",
       "      <td>-0.026766</td>\n",
       "      <td>-0.040522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.016481</td>\n",
       "      <td>0.097511</td>\n",
       "      <td>0.016481</td>\n",
       "      <td>0.128380</td>\n",
       "      <td>0.629087</td>\n",
       "      <td>-0.005101</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>0.010770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.102585</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.129606</td>\n",
       "      <td>0.568207</td>\n",
       "      <td>-0.024387</td>\n",
       "      <td>-0.048023</td>\n",
       "      <td>-0.052256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.098257</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.128068</td>\n",
       "      <td>0.615558</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>0.039759</td>\n",
       "      <td>0.068822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.098671</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.128059</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.028103</td>\n",
       "      <td>-0.034248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>0.105535</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>0.131427</td>\n",
       "      <td>0.532131</td>\n",
       "      <td>-0.053383</td>\n",
       "      <td>-0.020582</td>\n",
       "      <td>-0.041967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.098622</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.128056</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.040034</td>\n",
       "      <td>-0.054606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.099193</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.128133</td>\n",
       "      <td>0.602029</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>-0.043218</td>\n",
       "      <td>-0.082538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.098256</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.128068</td>\n",
       "      <td>0.615558</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.053409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.016405</td>\n",
       "      <td>0.098166</td>\n",
       "      <td>0.016405</td>\n",
       "      <td>0.128082</td>\n",
       "      <td>0.624577</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>-0.020786</td>\n",
       "      <td>-0.013038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>0.101228</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>0.128892</td>\n",
       "      <td>0.587373</td>\n",
       "      <td>-0.013137</td>\n",
       "      <td>0.051786</td>\n",
       "      <td>0.081752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>0.097341</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>0.128559</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>-0.007905</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>-0.022895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.097407</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.128482</td>\n",
       "      <td>0.627959</td>\n",
       "      <td>-0.006709</td>\n",
       "      <td>-0.006871</td>\n",
       "      <td>0.015614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.097646</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.128283</td>\n",
       "      <td>0.632469</td>\n",
       "      <td>-0.003584</td>\n",
       "      <td>-0.067154</td>\n",
       "      <td>-0.072579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>0.100172</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>0.128430</td>\n",
       "      <td>0.591883</td>\n",
       "      <td>-0.005883</td>\n",
       "      <td>-0.070716</td>\n",
       "      <td>-0.037955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.098925</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.128086</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.040951</td>\n",
       "      <td>-0.047534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.097784</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.128206</td>\n",
       "      <td>0.629087</td>\n",
       "      <td>-0.002384</td>\n",
       "      <td>-0.069352</td>\n",
       "      <td>-0.074078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.098279</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.128065</td>\n",
       "      <td>0.615558</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.110191</td>\n",
       "      <td>-0.122078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.099319</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.128162</td>\n",
       "      <td>0.593010</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.097633</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.629087</td>\n",
       "      <td>-0.003717</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.037308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.016411</td>\n",
       "      <td>0.098069</td>\n",
       "      <td>0.016411</td>\n",
       "      <td>0.128104</td>\n",
       "      <td>0.624577</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>0.043747</td>\n",
       "      <td>0.020655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.016552</td>\n",
       "      <td>0.100724</td>\n",
       "      <td>0.016552</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.586246</td>\n",
       "      <td>-0.009418</td>\n",
       "      <td>0.082265</td>\n",
       "      <td>0.066025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>0.099864</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>0.128319</td>\n",
       "      <td>0.589628</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>0.023909</td>\n",
       "      <td>-0.002974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>0.099902</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>0.128332</td>\n",
       "      <td>0.589628</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.028510</td>\n",
       "      <td>0.070984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.098138</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.128088</td>\n",
       "      <td>0.624577</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>-0.071822</td>\n",
       "      <td>-0.050655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.128072</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.040487</td>\n",
       "      <td>-0.025860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>0.099503</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>0.128209</td>\n",
       "      <td>0.593010</td>\n",
       "      <td>-0.002436</td>\n",
       "      <td>-0.072734</td>\n",
       "      <td>-0.080457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.098676</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.128060</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.098521</td>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.128054</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.021463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.098324</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.128061</td>\n",
       "      <td>0.615558</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>0.033131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.128061</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.052062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.098815</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.128072</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.004957</td>\n",
       "      <td>-0.011087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.099103</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.128115</td>\n",
       "      <td>0.600902</td>\n",
       "      <td>-0.000965</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>-0.012444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.016411</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.016411</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.024259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.016410</td>\n",
       "      <td>0.099029</td>\n",
       "      <td>0.016410</td>\n",
       "      <td>0.128102</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.059879</td>\n",
       "      <td>-0.056624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.016409</td>\n",
       "      <td>0.099013</td>\n",
       "      <td>0.016409</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.603157</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>0.072879</td>\n",
       "      <td>0.081731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n",
      "/tmp/ipykernel_53082/483633152.py:21: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  pearson_corr, _ = pearsonr(predictions, labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9040, training_loss=0.01948209975277428, metrics={'train_runtime': 1104.6084, 'train_samples_per_second': 65.471, 'train_steps_per_second': 8.184, 'total_flos': 16671367788864.0, 'train_loss': 0.01948209975277428, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac787d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
