{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b47c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#load train data\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets  = load_dataset(\"glue\", 'stsb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de228bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-large\"\n",
    "\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed721fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx', 'labels', 'input'],\n",
      "    num_rows: 5749\n",
      "})\n",
      "Validation Dataset: Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx', 'labels', 'input'],\n",
      "    num_rows: 1500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a prompt for evaluating the humor intensity of an edited headline.\n",
    "    Args:\n",
    "        data_point (dict): A dictionary containing 'original', 'edit', and 'meanGrade'.\n",
    "    Returns:\n",
    "        str: The formatted prompt as a string.\n",
    "    \"\"\"\n",
    "    return f\"\"\"# Sentence-1:: {data_point['sentence1']}. # Sentence-2: {data_point['sentence2']} # Output: The similarity is\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "def add_label_column(example):\n",
    "\n",
    "    example['labels'] = float(example['label'])\n",
    "  \n",
    "    example['input'] = generate_prompt(example)\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Map the function over train and validation datasets\n",
    "\n",
    "train_data = raw_datasets['train'].map(add_label_column)\n",
    "val_data = raw_datasets['validation'].map(add_label_column)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "\n",
    "# Inspect the updated datasets\n",
    "print(\"Train Dataset:\", train_data)\n",
    "print(\"Validation Dataset:\", val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33204c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fde6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5749/5749 [00:00<00:00, 21742.67 examples/s]\n",
      "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 20928.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete =  ['sentence1', 'sentence2', 'label', 'idx', 'input']\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "def preprocessing_function(examples):\n",
    "   \n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_data = train_data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_val_data = val_data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_train_data.set_format(\"torch\")\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1931ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]# Sentence-1:: The man hit the other man with a stick.. # Sentence-2: The man spanked the other man with a stick. # Output: The similarity is[SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train_data['input_ids'][10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6b985",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25900f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'labels', 'input'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdaa612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lengths = [len(ids) for ids in tokenized_train_data['input_ids']]\n",
    "mx = max(all_lengths)\n",
    "mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6618d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = sum(len(ids) > 512 for ids in tokenized_train_data['input_ids'])\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a46cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "# from modeling import MLMSequenceClassification\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.mask_token_id = tokenizer.mask_token_id\n",
    "config.num_labels = 1\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159b238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertForSequenceClassification(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(50368, 1024, padding_idx=50283)\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-27): 27 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864ccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RoCoFT\n",
    "\n",
    "RoCoFT.PEFT(model, method='column', rank=3) \n",
    "#targets=['key', 'value', 'dense', 'query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef34afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # If predictions are logits or have extra dimensions, squeeze\n",
    "    if predictions.ndim > 1:\n",
    "        predictions = predictions.squeeze()\n",
    "\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    \n",
    "    # Define an \"accuracy\" for regression:\n",
    "    # Example: within some threshold tolerance\n",
    "    tolerance = 0.1  # you can change this\n",
    "    acc = np.mean(np.abs(predictions - labels) < tolerance)\n",
    "\n",
    "    pearson_corr, _ = pearsonr(predictions, labels)\n",
    "    spearman_corr, _ = spearmanr(predictions, labels)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Accuracy\": acc,\n",
    "        \"R2\": r2,\n",
    "        \"Pearson\": pearson_corr,\n",
    "        \"Spearman's Rank\": spearman_corr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbcf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=6e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.20,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "557cdbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/emnlp_2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7200' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7200/7200 1:03:53, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>R2</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.245100</td>\n",
       "      <td>1.711111</td>\n",
       "      <td>1.084861</td>\n",
       "      <td>1.711111</td>\n",
       "      <td>1.308094</td>\n",
       "      <td>0.054667</td>\n",
       "      <td>0.239492</td>\n",
       "      <td>0.615992</td>\n",
       "      <td>0.645623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.011700</td>\n",
       "      <td>0.701396</td>\n",
       "      <td>0.665862</td>\n",
       "      <td>0.701396</td>\n",
       "      <td>0.837494</td>\n",
       "      <td>0.078667</td>\n",
       "      <td>0.688262</td>\n",
       "      <td>0.831216</td>\n",
       "      <td>0.834543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>0.572833</td>\n",
       "      <td>0.597718</td>\n",
       "      <td>0.572833</td>\n",
       "      <td>0.756857</td>\n",
       "      <td>0.101333</td>\n",
       "      <td>0.745403</td>\n",
       "      <td>0.871428</td>\n",
       "      <td>0.871292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.493016</td>\n",
       "      <td>0.554198</td>\n",
       "      <td>0.493016</td>\n",
       "      <td>0.702151</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.780877</td>\n",
       "      <td>0.886540</td>\n",
       "      <td>0.887002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.439594</td>\n",
       "      <td>0.511158</td>\n",
       "      <td>0.439594</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.804621</td>\n",
       "      <td>0.899021</td>\n",
       "      <td>0.897723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.412547</td>\n",
       "      <td>0.503664</td>\n",
       "      <td>0.412547</td>\n",
       "      <td>0.642298</td>\n",
       "      <td>0.125333</td>\n",
       "      <td>0.816642</td>\n",
       "      <td>0.905180</td>\n",
       "      <td>0.903824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.441460</td>\n",
       "      <td>0.526635</td>\n",
       "      <td>0.441460</td>\n",
       "      <td>0.664425</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.803792</td>\n",
       "      <td>0.902609</td>\n",
       "      <td>0.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.389099</td>\n",
       "      <td>0.479960</td>\n",
       "      <td>0.389099</td>\n",
       "      <td>0.623778</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.827064</td>\n",
       "      <td>0.910563</td>\n",
       "      <td>0.910030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.398560</td>\n",
       "      <td>0.489870</td>\n",
       "      <td>0.398560</td>\n",
       "      <td>0.631316</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.822859</td>\n",
       "      <td>0.909557</td>\n",
       "      <td>0.909223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.394472</td>\n",
       "      <td>0.479029</td>\n",
       "      <td>0.394472</td>\n",
       "      <td>0.628070</td>\n",
       "      <td>0.163333</td>\n",
       "      <td>0.824676</td>\n",
       "      <td>0.909501</td>\n",
       "      <td>0.909612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.382990</td>\n",
       "      <td>0.478010</td>\n",
       "      <td>0.382990</td>\n",
       "      <td>0.618862</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.829779</td>\n",
       "      <td>0.915005</td>\n",
       "      <td>0.912775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.384437</td>\n",
       "      <td>0.482767</td>\n",
       "      <td>0.384437</td>\n",
       "      <td>0.620030</td>\n",
       "      <td>0.125333</td>\n",
       "      <td>0.829136</td>\n",
       "      <td>0.911879</td>\n",
       "      <td>0.911255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.410386</td>\n",
       "      <td>0.499608</td>\n",
       "      <td>0.410386</td>\n",
       "      <td>0.640614</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.817603</td>\n",
       "      <td>0.909424</td>\n",
       "      <td>0.908449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.393844</td>\n",
       "      <td>0.489834</td>\n",
       "      <td>0.393844</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.824955</td>\n",
       "      <td>0.909846</td>\n",
       "      <td>0.910124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.389071</td>\n",
       "      <td>0.484549</td>\n",
       "      <td>0.389071</td>\n",
       "      <td>0.623755</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.827076</td>\n",
       "      <td>0.909926</td>\n",
       "      <td>0.908814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.407164</td>\n",
       "      <td>0.490249</td>\n",
       "      <td>0.407164</td>\n",
       "      <td>0.638094</td>\n",
       "      <td>0.157333</td>\n",
       "      <td>0.819035</td>\n",
       "      <td>0.907783</td>\n",
       "      <td>0.907347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.412821</td>\n",
       "      <td>0.503470</td>\n",
       "      <td>0.412821</td>\n",
       "      <td>0.642511</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.816520</td>\n",
       "      <td>0.910720</td>\n",
       "      <td>0.908530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.412628</td>\n",
       "      <td>0.501194</td>\n",
       "      <td>0.412628</td>\n",
       "      <td>0.642361</td>\n",
       "      <td>0.129333</td>\n",
       "      <td>0.816606</td>\n",
       "      <td>0.909572</td>\n",
       "      <td>0.908210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.393072</td>\n",
       "      <td>0.485295</td>\n",
       "      <td>0.393072</td>\n",
       "      <td>0.626955</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.910304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.406361</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>0.406361</td>\n",
       "      <td>0.637465</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0.819391</td>\n",
       "      <td>0.911728</td>\n",
       "      <td>0.909058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.399484</td>\n",
       "      <td>0.486273</td>\n",
       "      <td>0.399484</td>\n",
       "      <td>0.632047</td>\n",
       "      <td>0.159333</td>\n",
       "      <td>0.822448</td>\n",
       "      <td>0.906919</td>\n",
       "      <td>0.906539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.409605</td>\n",
       "      <td>0.493160</td>\n",
       "      <td>0.409605</td>\n",
       "      <td>0.640004</td>\n",
       "      <td>0.149333</td>\n",
       "      <td>0.817950</td>\n",
       "      <td>0.905698</td>\n",
       "      <td>0.904484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.404497</td>\n",
       "      <td>0.491424</td>\n",
       "      <td>0.404497</td>\n",
       "      <td>0.636001</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.820220</td>\n",
       "      <td>0.907107</td>\n",
       "      <td>0.905976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.401398</td>\n",
       "      <td>0.488428</td>\n",
       "      <td>0.401398</td>\n",
       "      <td>0.633560</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.821597</td>\n",
       "      <td>0.906511</td>\n",
       "      <td>0.906619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.397557</td>\n",
       "      <td>0.490184</td>\n",
       "      <td>0.397557</td>\n",
       "      <td>0.630522</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.823304</td>\n",
       "      <td>0.908453</td>\n",
       "      <td>0.907201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.391147</td>\n",
       "      <td>0.485046</td>\n",
       "      <td>0.391147</td>\n",
       "      <td>0.625417</td>\n",
       "      <td>0.138667</td>\n",
       "      <td>0.826154</td>\n",
       "      <td>0.908947</td>\n",
       "      <td>0.907658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.399788</td>\n",
       "      <td>0.484323</td>\n",
       "      <td>0.399788</td>\n",
       "      <td>0.632288</td>\n",
       "      <td>0.154667</td>\n",
       "      <td>0.822313</td>\n",
       "      <td>0.908439</td>\n",
       "      <td>0.907101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.487338</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.632238</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.822341</td>\n",
       "      <td>0.908105</td>\n",
       "      <td>0.906265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.396088</td>\n",
       "      <td>0.483904</td>\n",
       "      <td>0.396088</td>\n",
       "      <td>0.629355</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>0.823958</td>\n",
       "      <td>0.908167</td>\n",
       "      <td>0.906772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.406792</td>\n",
       "      <td>0.495271</td>\n",
       "      <td>0.406792</td>\n",
       "      <td>0.637803</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>0.905353</td>\n",
       "      <td>0.903985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.497658</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.641849</td>\n",
       "      <td>0.149333</td>\n",
       "      <td>0.816899</td>\n",
       "      <td>0.905074</td>\n",
       "      <td>0.903959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.405366</td>\n",
       "      <td>0.493583</td>\n",
       "      <td>0.405366</td>\n",
       "      <td>0.636683</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.819834</td>\n",
       "      <td>0.906257</td>\n",
       "      <td>0.905554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.405474</td>\n",
       "      <td>0.492554</td>\n",
       "      <td>0.405474</td>\n",
       "      <td>0.636769</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.819786</td>\n",
       "      <td>0.905903</td>\n",
       "      <td>0.904529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.402916</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>0.402916</td>\n",
       "      <td>0.634757</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>0.906792</td>\n",
       "      <td>0.905571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.490264</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.819997</td>\n",
       "      <td>0.906779</td>\n",
       "      <td>0.905775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.406455</td>\n",
       "      <td>0.493945</td>\n",
       "      <td>0.406455</td>\n",
       "      <td>0.637538</td>\n",
       "      <td>0.148667</td>\n",
       "      <td>0.819350</td>\n",
       "      <td>0.905509</td>\n",
       "      <td>0.904463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.416588</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.416588</td>\n",
       "      <td>0.645436</td>\n",
       "      <td>0.155333</td>\n",
       "      <td>0.814846</td>\n",
       "      <td>0.905735</td>\n",
       "      <td>0.904355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.409059</td>\n",
       "      <td>0.490532</td>\n",
       "      <td>0.409059</td>\n",
       "      <td>0.639578</td>\n",
       "      <td>0.159333</td>\n",
       "      <td>0.818192</td>\n",
       "      <td>0.906017</td>\n",
       "      <td>0.904779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.407785</td>\n",
       "      <td>0.493037</td>\n",
       "      <td>0.407785</td>\n",
       "      <td>0.638580</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.818759</td>\n",
       "      <td>0.905343</td>\n",
       "      <td>0.904048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.409146</td>\n",
       "      <td>0.494892</td>\n",
       "      <td>0.409146</td>\n",
       "      <td>0.639645</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.818154</td>\n",
       "      <td>0.904711</td>\n",
       "      <td>0.903347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.405271</td>\n",
       "      <td>0.490842</td>\n",
       "      <td>0.405271</td>\n",
       "      <td>0.636609</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.819876</td>\n",
       "      <td>0.905781</td>\n",
       "      <td>0.904120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.410206</td>\n",
       "      <td>0.494144</td>\n",
       "      <td>0.410206</td>\n",
       "      <td>0.640473</td>\n",
       "      <td>0.151333</td>\n",
       "      <td>0.817683</td>\n",
       "      <td>0.905624</td>\n",
       "      <td>0.904691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.410264</td>\n",
       "      <td>0.494103</td>\n",
       "      <td>0.410264</td>\n",
       "      <td>0.640518</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.817657</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.903656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.409238</td>\n",
       "      <td>0.493560</td>\n",
       "      <td>0.409238</td>\n",
       "      <td>0.639717</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.818113</td>\n",
       "      <td>0.905027</td>\n",
       "      <td>0.903749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.404230</td>\n",
       "      <td>0.491053</td>\n",
       "      <td>0.404230</td>\n",
       "      <td>0.635791</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.820339</td>\n",
       "      <td>0.905836</td>\n",
       "      <td>0.904598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.408771</td>\n",
       "      <td>0.492440</td>\n",
       "      <td>0.408771</td>\n",
       "      <td>0.639352</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.818320</td>\n",
       "      <td>0.905176</td>\n",
       "      <td>0.903908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.405173</td>\n",
       "      <td>0.491982</td>\n",
       "      <td>0.405173</td>\n",
       "      <td>0.636532</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.819920</td>\n",
       "      <td>0.905658</td>\n",
       "      <td>0.904528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.410774</td>\n",
       "      <td>0.494014</td>\n",
       "      <td>0.410774</td>\n",
       "      <td>0.640916</td>\n",
       "      <td>0.140667</td>\n",
       "      <td>0.817430</td>\n",
       "      <td>0.904485</td>\n",
       "      <td>0.903689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.494693</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.641774</td>\n",
       "      <td>0.149333</td>\n",
       "      <td>0.816942</td>\n",
       "      <td>0.904567</td>\n",
       "      <td>0.903658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.494239</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.640627</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.817595</td>\n",
       "      <td>0.904768</td>\n",
       "      <td>0.903727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.411804</td>\n",
       "      <td>0.495067</td>\n",
       "      <td>0.411804</td>\n",
       "      <td>0.641720</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.816972</td>\n",
       "      <td>0.904247</td>\n",
       "      <td>0.902920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.411177</td>\n",
       "      <td>0.496755</td>\n",
       "      <td>0.411177</td>\n",
       "      <td>0.641231</td>\n",
       "      <td>0.137333</td>\n",
       "      <td>0.817251</td>\n",
       "      <td>0.904478</td>\n",
       "      <td>0.903187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.411171</td>\n",
       "      <td>0.495041</td>\n",
       "      <td>0.411171</td>\n",
       "      <td>0.641226</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.817254</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.902993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.410238</td>\n",
       "      <td>0.494881</td>\n",
       "      <td>0.410238</td>\n",
       "      <td>0.640498</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.817668</td>\n",
       "      <td>0.904569</td>\n",
       "      <td>0.903433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.497446</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.643428</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.815996</td>\n",
       "      <td>0.903643</td>\n",
       "      <td>0.902560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.411720</td>\n",
       "      <td>0.495705</td>\n",
       "      <td>0.411720</td>\n",
       "      <td>0.641654</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.817010</td>\n",
       "      <td>0.904323</td>\n",
       "      <td>0.903026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.412147</td>\n",
       "      <td>0.495682</td>\n",
       "      <td>0.412147</td>\n",
       "      <td>0.641987</td>\n",
       "      <td>0.148667</td>\n",
       "      <td>0.816820</td>\n",
       "      <td>0.904122</td>\n",
       "      <td>0.902893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.412323</td>\n",
       "      <td>0.495851</td>\n",
       "      <td>0.412323</td>\n",
       "      <td>0.642124</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.904151</td>\n",
       "      <td>0.902815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.412636</td>\n",
       "      <td>0.496471</td>\n",
       "      <td>0.412636</td>\n",
       "      <td>0.642368</td>\n",
       "      <td>0.148667</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>0.903923</td>\n",
       "      <td>0.902758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.412283</td>\n",
       "      <td>0.495780</td>\n",
       "      <td>0.412283</td>\n",
       "      <td>0.642092</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.816760</td>\n",
       "      <td>0.904248</td>\n",
       "      <td>0.902879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.412307</td>\n",
       "      <td>0.495793</td>\n",
       "      <td>0.412307</td>\n",
       "      <td>0.642111</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.816749</td>\n",
       "      <td>0.904190</td>\n",
       "      <td>0.902919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.412901</td>\n",
       "      <td>0.496433</td>\n",
       "      <td>0.412901</td>\n",
       "      <td>0.642574</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.816485</td>\n",
       "      <td>0.903945</td>\n",
       "      <td>0.902799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.413181</td>\n",
       "      <td>0.496207</td>\n",
       "      <td>0.413181</td>\n",
       "      <td>0.642792</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.816360</td>\n",
       "      <td>0.903966</td>\n",
       "      <td>0.902798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.413031</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.413031</td>\n",
       "      <td>0.642675</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.816427</td>\n",
       "      <td>0.903999</td>\n",
       "      <td>0.902786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.412965</td>\n",
       "      <td>0.496397</td>\n",
       "      <td>0.412965</td>\n",
       "      <td>0.642624</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.816456</td>\n",
       "      <td>0.903969</td>\n",
       "      <td>0.902818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.413097</td>\n",
       "      <td>0.496362</td>\n",
       "      <td>0.413097</td>\n",
       "      <td>0.642726</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.816398</td>\n",
       "      <td>0.903943</td>\n",
       "      <td>0.902801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.412984</td>\n",
       "      <td>0.496470</td>\n",
       "      <td>0.412984</td>\n",
       "      <td>0.642638</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.816448</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.902788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.412832</td>\n",
       "      <td>0.496292</td>\n",
       "      <td>0.412832</td>\n",
       "      <td>0.642520</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.816516</td>\n",
       "      <td>0.903986</td>\n",
       "      <td>0.902848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.413059</td>\n",
       "      <td>0.496443</td>\n",
       "      <td>0.413059</td>\n",
       "      <td>0.642697</td>\n",
       "      <td>0.142667</td>\n",
       "      <td>0.816415</td>\n",
       "      <td>0.903930</td>\n",
       "      <td>0.902788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.413034</td>\n",
       "      <td>0.496436</td>\n",
       "      <td>0.413034</td>\n",
       "      <td>0.642677</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.816426</td>\n",
       "      <td>0.903925</td>\n",
       "      <td>0.902805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.413146</td>\n",
       "      <td>0.496471</td>\n",
       "      <td>0.413146</td>\n",
       "      <td>0.642764</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.816376</td>\n",
       "      <td>0.903917</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.413165</td>\n",
       "      <td>0.496470</td>\n",
       "      <td>0.413165</td>\n",
       "      <td>0.642779</td>\n",
       "      <td>0.143333</td>\n",
       "      <td>0.816368</td>\n",
       "      <td>0.903916</td>\n",
       "      <td>0.902779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7200, training_loss=0.14480090303243034, metrics={'train_runtime': 3842.2734, 'train_samples_per_second': 29.925, 'train_steps_per_second': 1.874, 'total_flos': 51695103825960.0, 'train_loss': 0.14480090303243034, 'epoch': 20.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e83df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
