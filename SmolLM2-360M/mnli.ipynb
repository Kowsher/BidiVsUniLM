{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c500ac0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/emnlp_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "#from roberta import RobertaForSequenceClassification\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Log in using your Hugging Face token\n",
        "login(\"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\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5247e6d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets  = load_dataset(\"glue\", 'mnli')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "074409bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "#from roberta import RobertaForSequenceClassification\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM2-360M\"\n",
        "\n",
        "#config.num_labels=2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e0f5477",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "col_to_delete = ['question1','sentence2']\n",
        "\n",
        "def preprocessing_function(examples):\n",
        "    return tokenizer(examples['premise'], examples['hypothesis'])\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True)\n",
        "\n",
        "\n",
        "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "285ef2b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers.activations import ACT2FN\n",
        "import random\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to('cuda')\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "import RoCoFT\n",
        "\n",
        "RoCoFT.PEFT(model, method='column', rank=3) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "09b86e50",
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "\n",
        "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    \n",
        "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
        "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
        "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
        "    accuracy = metrics.accuracy_score(labels, predictions)\n",
        "    \n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "de592e9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "import time\n",
        "from transformers import Trainer, TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='dir',\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps= 4,\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    save_steps=10000000,\n",
        "    # weight_decay=0.1,\n",
        "    # warmup_ratio=0.1,\n",
        "    logging_steps=500,\n",
        "    max_grad_norm = 1,\n",
        "    load_best_model_at_end=True,\n",
        "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
        "    warmup_steps=500,\n",
        "    # label_smoothing_factor=0.1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12ccd2fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24544' max='24544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24544/24544 4:02:15, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-score</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.153500</td>\n",
              "      <td>0.922571</td>\n",
              "      <td>0.572534</td>\n",
              "      <td>0.570498</td>\n",
              "      <td>0.565412</td>\n",
              "      <td>0.572899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.794700</td>\n",
              "      <td>0.705117</td>\n",
              "      <td>0.707280</td>\n",
              "      <td>0.704554</td>\n",
              "      <td>0.705306</td>\n",
              "      <td>0.706164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.641500</td>\n",
              "      <td>0.590575</td>\n",
              "      <td>0.762618</td>\n",
              "      <td>0.755993</td>\n",
              "      <td>0.757004</td>\n",
              "      <td>0.756801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.565000</td>\n",
              "      <td>0.540294</td>\n",
              "      <td>0.782232</td>\n",
              "      <td>0.782493</td>\n",
              "      <td>0.782126</td>\n",
              "      <td>0.784717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.532000</td>\n",
              "      <td>0.510834</td>\n",
              "      <td>0.792650</td>\n",
              "      <td>0.793071</td>\n",
              "      <td>0.792400</td>\n",
              "      <td>0.794396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.495200</td>\n",
              "      <td>0.501075</td>\n",
              "      <td>0.803804</td>\n",
              "      <td>0.801727</td>\n",
              "      <td>0.801213</td>\n",
              "      <td>0.801325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.491100</td>\n",
              "      <td>0.467319</td>\n",
              "      <td>0.811710</td>\n",
              "      <td>0.811673</td>\n",
              "      <td>0.811414</td>\n",
              "      <td>0.813653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.473900</td>\n",
              "      <td>0.464558</td>\n",
              "      <td>0.819246</td>\n",
              "      <td>0.818457</td>\n",
              "      <td>0.818218</td>\n",
              "      <td>0.818645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.454700</td>\n",
              "      <td>0.459935</td>\n",
              "      <td>0.820171</td>\n",
              "      <td>0.818267</td>\n",
              "      <td>0.818087</td>\n",
              "      <td>0.818034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.453400</td>\n",
              "      <td>0.443593</td>\n",
              "      <td>0.823507</td>\n",
              "      <td>0.823775</td>\n",
              "      <td>0.823375</td>\n",
              "      <td>0.824452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.447400</td>\n",
              "      <td>0.436753</td>\n",
              "      <td>0.829118</td>\n",
              "      <td>0.828402</td>\n",
              "      <td>0.828502</td>\n",
              "      <td>0.829037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.445500</td>\n",
              "      <td>0.427799</td>\n",
              "      <td>0.831754</td>\n",
              "      <td>0.830935</td>\n",
              "      <td>0.831105</td>\n",
              "      <td>0.831686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.436100</td>\n",
              "      <td>0.426823</td>\n",
              "      <td>0.831012</td>\n",
              "      <td>0.831419</td>\n",
              "      <td>0.830976</td>\n",
              "      <td>0.832399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.421500</td>\n",
              "      <td>0.426139</td>\n",
              "      <td>0.832074</td>\n",
              "      <td>0.832019</td>\n",
              "      <td>0.831704</td>\n",
              "      <td>0.832399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.423000</td>\n",
              "      <td>0.417731</td>\n",
              "      <td>0.835091</td>\n",
              "      <td>0.835110</td>\n",
              "      <td>0.834700</td>\n",
              "      <td>0.835354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.427100</td>\n",
              "      <td>0.429415</td>\n",
              "      <td>0.831721</td>\n",
              "      <td>0.830469</td>\n",
              "      <td>0.829927</td>\n",
              "      <td>0.830056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.421200</td>\n",
              "      <td>0.409037</td>\n",
              "      <td>0.838681</td>\n",
              "      <td>0.838683</td>\n",
              "      <td>0.838561</td>\n",
              "      <td>0.839328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.420300</td>\n",
              "      <td>0.410133</td>\n",
              "      <td>0.838750</td>\n",
              "      <td>0.838935</td>\n",
              "      <td>0.838541</td>\n",
              "      <td>0.839328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.403538</td>\n",
              "      <td>0.838189</td>\n",
              "      <td>0.837923</td>\n",
              "      <td>0.837875</td>\n",
              "      <td>0.839531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.403000</td>\n",
              "      <td>0.402356</td>\n",
              "      <td>0.843083</td>\n",
              "      <td>0.842865</td>\n",
              "      <td>0.842868</td>\n",
              "      <td>0.843505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.414200</td>\n",
              "      <td>0.405594</td>\n",
              "      <td>0.843012</td>\n",
              "      <td>0.842170</td>\n",
              "      <td>0.841890</td>\n",
              "      <td>0.842078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.411000</td>\n",
              "      <td>0.414663</td>\n",
              "      <td>0.841252</td>\n",
              "      <td>0.838837</td>\n",
              "      <td>0.838395</td>\n",
              "      <td>0.838105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.404200</td>\n",
              "      <td>0.395942</td>\n",
              "      <td>0.847342</td>\n",
              "      <td>0.847405</td>\n",
              "      <td>0.847188</td>\n",
              "      <td>0.847886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.403100</td>\n",
              "      <td>0.396940</td>\n",
              "      <td>0.846157</td>\n",
              "      <td>0.845284</td>\n",
              "      <td>0.845274</td>\n",
              "      <td>0.845543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.384400</td>\n",
              "      <td>0.399055</td>\n",
              "      <td>0.846496</td>\n",
              "      <td>0.846579</td>\n",
              "      <td>0.846234</td>\n",
              "      <td>0.846867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.367200</td>\n",
              "      <td>0.395382</td>\n",
              "      <td>0.849130</td>\n",
              "      <td>0.847317</td>\n",
              "      <td>0.847237</td>\n",
              "      <td>0.847173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.371800</td>\n",
              "      <td>0.391218</td>\n",
              "      <td>0.848827</td>\n",
              "      <td>0.849246</td>\n",
              "      <td>0.848871</td>\n",
              "      <td>0.850127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.370100</td>\n",
              "      <td>0.393360</td>\n",
              "      <td>0.851786</td>\n",
              "      <td>0.850556</td>\n",
              "      <td>0.850589</td>\n",
              "      <td>0.850739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.372900</td>\n",
              "      <td>0.386063</td>\n",
              "      <td>0.847553</td>\n",
              "      <td>0.847633</td>\n",
              "      <td>0.847576</td>\n",
              "      <td>0.848803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.369200</td>\n",
              "      <td>0.389876</td>\n",
              "      <td>0.849319</td>\n",
              "      <td>0.849445</td>\n",
              "      <td>0.849237</td>\n",
              "      <td>0.850025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.373400</td>\n",
              "      <td>0.387857</td>\n",
              "      <td>0.848711</td>\n",
              "      <td>0.848671</td>\n",
              "      <td>0.848645</td>\n",
              "      <td>0.849414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.367300</td>\n",
              "      <td>0.391532</td>\n",
              "      <td>0.847381</td>\n",
              "      <td>0.847416</td>\n",
              "      <td>0.847171</td>\n",
              "      <td>0.847784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.362500</td>\n",
              "      <td>0.392701</td>\n",
              "      <td>0.849379</td>\n",
              "      <td>0.849153</td>\n",
              "      <td>0.848952</td>\n",
              "      <td>0.849414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.378300</td>\n",
              "      <td>0.388003</td>\n",
              "      <td>0.849963</td>\n",
              "      <td>0.849390</td>\n",
              "      <td>0.849392</td>\n",
              "      <td>0.849822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.371600</td>\n",
              "      <td>0.390237</td>\n",
              "      <td>0.847026</td>\n",
              "      <td>0.847376</td>\n",
              "      <td>0.846967</td>\n",
              "      <td>0.847784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.362200</td>\n",
              "      <td>0.386194</td>\n",
              "      <td>0.849099</td>\n",
              "      <td>0.848788</td>\n",
              "      <td>0.848812</td>\n",
              "      <td>0.849414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.365700</td>\n",
              "      <td>0.389995</td>\n",
              "      <td>0.849012</td>\n",
              "      <td>0.848356</td>\n",
              "      <td>0.848281</td>\n",
              "      <td>0.848599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.362400</td>\n",
              "      <td>0.385880</td>\n",
              "      <td>0.848599</td>\n",
              "      <td>0.848649</td>\n",
              "      <td>0.848519</td>\n",
              "      <td>0.849210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.373500</td>\n",
              "      <td>0.386987</td>\n",
              "      <td>0.849676</td>\n",
              "      <td>0.849490</td>\n",
              "      <td>0.849484</td>\n",
              "      <td>0.850127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.368100</td>\n",
              "      <td>0.384914</td>\n",
              "      <td>0.849967</td>\n",
              "      <td>0.850027</td>\n",
              "      <td>0.849946</td>\n",
              "      <td>0.850739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.361700</td>\n",
              "      <td>0.384139</td>\n",
              "      <td>0.850695</td>\n",
              "      <td>0.850567</td>\n",
              "      <td>0.850559</td>\n",
              "      <td>0.851248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.361800</td>\n",
              "      <td>0.385055</td>\n",
              "      <td>0.849763</td>\n",
              "      <td>0.849488</td>\n",
              "      <td>0.849518</td>\n",
              "      <td>0.850127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.367700</td>\n",
              "      <td>0.386253</td>\n",
              "      <td>0.849675</td>\n",
              "      <td>0.849635</td>\n",
              "      <td>0.849554</td>\n",
              "      <td>0.850229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.355800</td>\n",
              "      <td>0.385441</td>\n",
              "      <td>0.848843</td>\n",
              "      <td>0.848969</td>\n",
              "      <td>0.848833</td>\n",
              "      <td>0.849618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.373300</td>\n",
              "      <td>0.385237</td>\n",
              "      <td>0.849044</td>\n",
              "      <td>0.849118</td>\n",
              "      <td>0.848979</td>\n",
              "      <td>0.849720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.361000</td>\n",
              "      <td>0.385167</td>\n",
              "      <td>0.849245</td>\n",
              "      <td>0.849258</td>\n",
              "      <td>0.849133</td>\n",
              "      <td>0.849822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.365600</td>\n",
              "      <td>0.385618</td>\n",
              "      <td>0.849554</td>\n",
              "      <td>0.849583</td>\n",
              "      <td>0.849437</td>\n",
              "      <td>0.850127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.365000</td>\n",
              "      <td>0.385627</td>\n",
              "      <td>0.850068</td>\n",
              "      <td>0.850017</td>\n",
              "      <td>0.849898</td>\n",
              "      <td>0.850535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.363700</td>\n",
              "      <td>0.385667</td>\n",
              "      <td>0.850068</td>\n",
              "      <td>0.850017</td>\n",
              "      <td>0.849898</td>\n",
              "      <td>0.850535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=24544, training_loss=0.4313935077174857, metrics={'train_runtime': 14536.1779, 'train_samples_per_second': 54.031, 'train_steps_per_second': 1.688, 'total_flos': 287194017644100.0, 'train_loss': 0.4313935077174857, 'epoch': 2.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76b7afa",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "emnlp_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
